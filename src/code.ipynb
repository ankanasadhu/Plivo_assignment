{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f461d7",
   "metadata": {},
   "source": [
    "### Generating randon set of data for training and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import string\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for Indian names/locations\n",
    "fake = Faker('en_IN')\n",
    "\n",
    "def get_spoken_digits(ph_str):\n",
    "    digit_map = {'0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four', '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'}\n",
    "    # fifty percent chance to write as words otherwise numbers\n",
    "    if random.choice([True, False]):\n",
    "        return \" \".join([digit_map[d] for d in ph_str])\n",
    "    return \" \".join(ph_str) \n",
    "\n",
    "def introduce_noise(text, noise_level=0.05):\n",
    "    if not text: # Handle empty string case\n",
    "        return text\n",
    "\n",
    "    noisy_text = list(text)\n",
    "    alphabet = string.ascii_lowercase + string.digits + ' '\n",
    "\n",
    "    # we either insert , delete or substitute random letters\n",
    "    for i in range(len(noisy_text) - 1, -1, -1):\n",
    "        if random.random() < noise_level:\n",
    "            action = random.choice(['insert', 'delete', 'substitute'])\n",
    "\n",
    "            if action == 'insert':\n",
    "                noisy_text.insert(i, random.choice(alphabet))\n",
    "            elif action == 'delete':\n",
    "                if len(noisy_text) > 1: # Don't delete if it's the last character\n",
    "                    noisy_text.pop(i)\n",
    "            elif action == 'substitute':\n",
    "                noisy_text[i] = random.choice(alphabet)\n",
    "    return \"\".join(noisy_text)\n",
    "\n",
    "def generate_entry(entry_id, noise_percentage=0.1):\n",
    "    # we generate patterns to mirror the dataset\n",
    "    patterns = [\n",
    "        \"email\", \"phone\", \"card\", \"mixed_travel\", \"simple_city\"\n",
    "    ]\n",
    "    pattern = random.choice(patterns)\n",
    "\n",
    "    text = \"\"\n",
    "    entities = []\n",
    "\n",
    "    if pattern == \"email\":\n",
    "        fname = fake.first_name().lower()\n",
    "        lname = fake.last_name().lower()\n",
    "        domain = random.choice([\"gmail\", \"outlook\", \"yahoo\", \"hotmail\"])\n",
    "\n",
    "        intro = random.choice([\"my email is \", \"email id is \", \"contact at \"])\n",
    "        p_name = f\"{fname} dot {lname}\"\n",
    "        email_domain = f\"{domain} dot com\"\n",
    "\n",
    "        # concatenate the random entities that are generated\n",
    "        full_str = f\"{intro}{p_name} at {email_domain}\"\n",
    "\n",
    "        p_start = len(intro)\n",
    "        p_end = p_start + len(p_name)\n",
    "\n",
    "        e_start = p_end + 4\n",
    "        e_end = e_start + len(email_domain)\n",
    "\n",
    "        text = full_str\n",
    "\n",
    "        entities.append({\"start\": p_start, \"end\": p_end, \"label\": \"PERSON_NAME\"})\n",
    "        entities.append({\"start\": e_start, \"end\": e_end, \"label\": \"EMAIL\"})\n",
    "\n",
    "    elif pattern == \"phone\":\n",
    "        ph = str(random.randint(6000000000, 9999999999))\n",
    "        ph_text = get_spoken_digits(ph)\n",
    "        intro = random.choice([\"my number is \", \"call me on \", \"phone \"])\n",
    "\n",
    "        text = intro + ph_text\n",
    "        entities.append({\"start\": len(intro), \"end\": len(text), \"label\": \"PHONE\"})\n",
    "\n",
    "    elif pattern == \"card\":\n",
    "        c1 = str(random.randint(1000,9999))\n",
    "        c2 = str(random.randint(1000,9999))\n",
    "        c3 = str(random.randint(1000,9999))\n",
    "        c4 = str(random.randint(1000,9999))\n",
    "        card_str = f\"{c1} {c2} {c3} {c4}\"\n",
    "        intro = random.choice([\"card number is \", \"my credit card is \", \"card \"])\n",
    "\n",
    "        text = intro + card_str\n",
    "        entities.append({\"start\": len(intro), \"end\": len(text), \"label\": \"CREDIT_CARD\"})\n",
    "\n",
    "    elif pattern == \"mixed_travel\":\n",
    "        city = fake.city().lower()\n",
    "        date_obj = fake.future_date()\n",
    "        date_str = date_obj.strftime(\"%d %m %Y\")\n",
    "\n",
    "        intro = \"i will travel to \"\n",
    "        mid = \" on \"\n",
    "\n",
    "        text = f\"{intro}{city}{mid}{date_str}\"\n",
    "\n",
    "        c_start = len(intro)\n",
    "        c_end = c_start + len(city)\n",
    "        d_start = c_end + len(mid)\n",
    "        d_end = d_start + len(date_str)\n",
    "\n",
    "        entities.append({\"start\": c_start, \"end\": c_end, \"label\": \"CITY\"})\n",
    "        entities.append({\"start\": d_start, \"end\": d_end, \"label\": \"DATE\"})\n",
    "\n",
    "    elif pattern == \"simple_city\":\n",
    "        city = fake.city().lower()\n",
    "        intro = random.choice([\"i live in \", \"location is \", \"from \"])\n",
    "        text = intro + city\n",
    "        entities.append({\"start\": len(intro), \"end\": len(text), \"label\": \"CITY\"})\n",
    "\n",
    "    # Introduce noise for a certain percentage of entries\n",
    "    if random.random() < noise_percentage:\n",
    "        original_text = text\n",
    "        text = introduce_noise(text)\n",
    "\n",
    "        entities = [] \n",
    "\n",
    "    return json.dumps({\"id\": entry_id, \"text\": text, \"entities\": entities})\n",
    "\n",
    "# Generate Files\n",
    "print(\"Generating train.jsonl (1000 lines)...\")\n",
    "with open(\"../data/train.jsonl\", \"w\") as f:\n",
    "    for i in range(1000):\n",
    "        # Pass noise_percentage to generate_entry\n",
    "        f.write(generate_entry(f\"utt_{i:04d}\", noise_percentage=0.3) + \"\\n\") # 30% noisy data\n",
    "\n",
    "print(\"Generating dev.jsonl (200 lines)...\")\n",
    "with open(\"../data/dev.jsonl\", \"w\") as f:\n",
    "    for i in range(200):\n",
    "        f.write(generate_entry(f\"utt_{1000+i:04d}\", noise_percentage=0.3) + \"\\n\") # 30% noisy data\n",
    "\n",
    "print(\"Done! Files saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090b0b9",
   "metadata": {},
   "source": [
    "### Running the training and evaluation pipeline using a larger model (BERT-base-NER) with the same noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4052c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([15]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████████████████████████| 125/125 [00:19<00:00,  6.25it/s]\n",
      "Epoch 1 average loss: 0.7821\n",
      "Epoch 2/3: 100%|██████████████████████████████| 125/125 [00:18<00:00,  6.67it/s]\n",
      "Epoch 2 average loss: 0.1975\n",
      "Epoch 3/3: 100%|██████████████████████████████| 125/125 [00:19<00:00,  6.47it/s]\n",
      "Epoch 3 average loss: 0.1405\n",
      "Saved model + tokenizer to ../out\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model_name dslim/bert-base-NER --train ../data/train.jsonl --dev ../data/dev.jsonl --out_dir ../out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6b8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions for 200 utterances to ../out/dev_pred.json\n"
     ]
    }
   ],
   "source": [
    "!python predict.py --model_dir ../out --input ../data/dev.jsonl --output ../out/dev_pred.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb390c",
   "metadata": {},
   "source": [
    "## We can clearly observe that the larger model gives a higher precision on the same dev data for the same hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdde5eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-entity metrics:\n",
      "CITY            P=0.850 R=0.962 F1=0.903\n",
      "CREDIT_CARD     P=0.857 R=1.000 F1=0.923\n",
      "DATE            P=0.926 R=1.000 F1=0.962\n",
      "EMAIL           P=0.909 R=1.000 F1=0.952\n",
      "PERSON_NAME     P=0.909 R=1.000 F1=0.952\n",
      "PHONE           P=0.800 R=1.000 F1=0.889\n",
      "\n",
      "Macro-F1: 0.930\n",
      "\n",
      "PII-only metrics: P=0.884 R=1.000 F1=0.938\n",
      "Non-PII metrics: P=0.850 R=0.962 F1=0.903\n"
     ]
    }
   ],
   "source": [
    "!python eval_span_f1.py --gold ../data/dev.jsonl --pred ../out/dev_pred.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694ae2b",
   "metadata": {},
   "source": [
    "### But the tradeoff can be clearly seen in the latency which is higher than 20ms for p95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd8c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency over 50 runs (batch_size=1):\n",
      "  p50: 28.73 ms\n",
      "  p95: 39.60 ms\n"
     ]
    }
   ],
   "source": [
    "!python measure_latency.py --model_dir ../out --input ../data/dev.jsonl --runs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb23206",
   "metadata": {},
   "source": [
    "### Here we train the default model with the same train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1c06ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████████████████████████| 125/125 [00:13<00:00,  8.97it/s]\n",
      "Epoch 1 average loss: 0.8492\n",
      "Epoch 2/3: 100%|██████████████████████████████| 125/125 [00:10<00:00, 12.27it/s]\n",
      "Epoch 2 average loss: 0.2047\n",
      "Epoch 3/3: 100%|██████████████████████████████| 125/125 [00:10<00:00, 11.48it/s]\n",
      "Epoch 3 average loss: 0.1298\n",
      "Saved model + tokenizer to ../out\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model_name distilbert-base-uncased --train ../data/train.jsonl --dev ../data/dev.jsonl --out_dir ../out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3b994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions for 200 utterances to ../out/dev_pred.json\n"
     ]
    }
   ],
   "source": [
    "!python predict.py --model_dir ../out --input ../data/dev.jsonl --output ../out/dev_pred.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e8f11",
   "metadata": {},
   "source": [
    "### The precision drops for the same hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a935c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-entity metrics:\n",
      "CITY            P=0.714 R=0.849 F1=0.776\n",
      "CREDIT_CARD     P=0.821 R=0.958 F1=0.885\n",
      "DATE            P=0.926 R=1.000 F1=0.962\n",
      "EMAIL           P=0.938 R=1.000 F1=0.968\n",
      "PERSON_NAME     P=0.882 R=1.000 F1=0.938\n",
      "PHONE           P=0.833 R=1.000 F1=0.909\n",
      "\n",
      "Macro-F1: 0.906\n",
      "\n",
      "PII-only metrics: P=0.883 R=0.992 F1=0.934\n",
      "Non-PII metrics: P=0.714 R=0.849 F1=0.776\n"
     ]
    }
   ],
   "source": [
    "!python eval_span_f1.py --gold ../data/dev.jsonl --pred ../out/dev_pred.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6883363",
   "metadata": {},
   "source": [
    "### With a relatively similar precision, the latency is extremely low for the smaller bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a643b4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency over 50 runs (batch_size=1):\n",
      "  p50: 12.52 ms\n",
      "  p95: 14.62 ms\n"
     ]
    }
   ],
   "source": [
    "!python measure_latency.py --model_dir ../out --input ../data/dev.jsonl --runs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3da1c",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "We need to tradeoff between the precision and inference latency. If we need better precision over the same test set, we use a slighlty larger model. But, if we need faster inference, we use a smaller model while losing some precision on the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbd347",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
